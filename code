import string
from sklearn.pipeline import Pipeline
import pandas as pd
import numpy as np

data = pd.read_csv("D:\Download dari Chrome\starbucksid_scrapped_data.csv", sep=",", encoding="utf-8")
data.head()

import re
def casefolding(comment):
    comment = comment.lower()
    comment = comment.strip(" ")
    comment = re.sub (r'[?|$|.|!Â²_:")(-+,]', '', comment)
    return comment
data['comment']= data['comment'].apply(casefolding)
data.head(100)

def token(comments):
    nstr = comments.split(' ')
    dat= []
    a = -1
    for hu in nstr:
        a = a + 1
    if hu == '':
        dat.append(a)
    p = 0
    b = 0
    for q in dat:
        b = q - p
        del nstr[b]
        p = p + 1
    return nstr
data['comment'] = data['comment'].apply(token)
data.head(10)

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

def stopword_removal (comments):
    filtering = stopwords.words('indonesian', 'english') 
    x = []
    data= []
    def myFunc(x): 
        if x in filtering: 
            return False
        else:
            return True
    fit = filter (myFunc, comments)
    for x in fit:
        data.append(x)
    return data
data['comment'] = data['comment'].apply(stopword_removal)
data.head()

from sklearn.pipeline import Pipeline
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

def stemming(comments):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    do = []
    for w in comments:
        dt = stemmer.stem(w)
        do.append(dt)
    d_clean = []
    d_clean = " ".join(do)
    print(d_clean)
    return d_clean
data['comment'] = data['comment'].apply(stemming)
data.to_csv('data_clean.csv', index=False)
data_clean = pd.read_csv('data_clean.csv', encoding='latin1')
data_clean.head()

data_clean = data_clean.astype({'VALUE' : 'category'})
data_clean = data_clean.astype({'comment' : 'string'})
data_clean.dtypes

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer()
text_tf = tf.fit_transform(data_clean['comment'].astype('U'))
text_tf

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(text_tf, data_clean['VALUE'], test_size=0.2, random_state = 42)

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

clf = MultinomialNB().fit(X_train, y_train)
predicted = clf.predict(X_test)
print("MultinomialNB Accuracy:", accuracy_score (y_test, predicted))
print("MultinomialNB Precision:", precision_score (y_test, predicted, average="binary", pos_label="Negatif" ))
print("MultinomialNB Recall:", recall_score (y_test, predicted, average="binary", pos_label="Negatif"))
print("MultinomialNB f1_score:", f1_score(y_test, predicted, average="binary", pos_label="Negatif"))

print(f'confusion matrix:\n {confusion_matrix(y_test, predicted)}')
print('================================================\n')
print(classification_report(y_test, predicted, zero_division=0))

